import requests
from bs4 import BeautifulSoup
import time
import random
import json
import os
from urllib.parse import unquote, urljoin

# ================= ğŸ“ ç”¨æˆ¶è¨­å®šå€ =================
# å°èªªæª”å (å¿…å¡«)
NOVEL_NAME = "AliceSW_Novel_31893" 

# å¦‚æœ JSON è£¡è®€ä¸åˆ°ç›®éŒ„ç¶²å€ï¼Œæ‰æœƒç”¨é€™å€‹å‚™ç”¨çš„
BACKUP_CATALOG_URL = "https://www.alicesw.com/other/chapters/id/31893.html" 

# è·³éå‰å¹¾ç«  (é è¨­ 45)
SKIP_COUNT = 0 
# ===============================================

BASE_URL = "https://www.alicesw.com"

def load_config_from_json(filename="cookie.json"):
    """
    å¾ cookie.json è‡ªå‹•è®€å– Cookie å­—ä¸²ï¼Œ
    ä¸¦å˜—è©¦å°‹æ‰¾æœ€å¾Œè¨ªå•çš„ç›®éŒ„é é¢ã€‚
    """
    if not os.path.exists(filename):
        print(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° {filename}ï¼Œè«‹ç¢ºèªæª”æ¡ˆä½ç½®ã€‚")
        return None, None

    try:
        with open(filename, 'r', encoding='utf-8') as f:
            data = json.load(f)

        # 1. çµ„åˆ Cookie å­—ä¸²
        cookie_parts = []
        target_url = None
        
        for item in data:
            name = item.get('name')
            value = item.get('value')
            if name and value:
                cookie_parts.append(f"{name}={value}")
                
            # 2. å˜—è©¦è‡ªå‹•æŠ“å–ç›®éŒ„ URL
            # åµæ¸¬ lf___forward__ æˆ–é¡ä¼¼æ¬„ä½
            if name == "lf___forward__" and "chapters/id" in unquote(value):
                # è§£ç¢¼ç¶²å€ (æŠŠ %2F è®Šå› /)
                relative_path = unquote(value)
                target_url = urljoin(BASE_URL, relative_path)
                print(f"ğŸ¯ è‡ªå‹•åµæ¸¬åˆ°ç›®éŒ„ç¶²å€: {target_url}")

        cookie_str = "; ".join(cookie_parts)
        return cookie_str, target_url

    except Exception as e:
        print(f"âŒ è§£æ JSON å¤±æ•—: {e}")
        return None, None

def get_soup(url, headers):
    for i in range(3):
        try:
            r = requests.get(url, headers=headers, timeout=15)
            if r.status_code == 200:
                r.encoding = 'utf-8'
                return BeautifulSoup(r.text, 'html.parser')
            elif r.status_code == 403:
                print("âŒ 403 Forbidden: Cookie å¯èƒ½å·²å¤±æ•ˆ")
                return None
            time.sleep(2)
        except Exception:
            time.sleep(2)
    return None

def main():
    print("ğŸ“‚ æ­£åœ¨è®€å– cookie.json ...")
    cookie_str, detected_url = load_config_from_json("cookie.json")
    
    if not cookie_str:
        return

    # æ±ºå®šä½¿ç”¨å“ªå€‹ç¶²å€ (è‡ªå‹•åµæ¸¬å„ªå…ˆï¼Œå¦å‰‡ç”¨å‚™ç”¨)
    catalog_url = detected_url if detected_url else BACKUP_CATALOG_URL
    print(f"ğŸš€ ç›®æ¨™ç›®éŒ„é : {catalog_url}")

    headers = {
        "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Cookie": cookie_str,
        "Referer": BASE_URL
    }

    soup = get_soup(catalog_url, headers)
    if not soup:
        print("âŒ ç„¡æ³•è®€å–ç›®éŒ„ï¼Œè«‹ç¢ºèª Cookie æ˜¯å¦æœ‰æ•ˆã€‚")
        return

    # æŠ“å–é€£çµ
    print("ğŸ” åˆ†æç« ç¯€åˆ—è¡¨...")
    links = soup.select("a")
    valid_chapters = []
    
    for link in links:
        href = link.get('href')
        title = link.get_text().strip()
        # éæ¿¾é‚è¼¯
        if href and "/book/" in href and len(title) > 1:
            full_url = urljoin(BASE_URL, href)
            if not any(c[1] == full_url for c in valid_chapters):
                valid_chapters.append((title, full_url))

    # è·³éå‰æ®µ
    if SKIP_COUNT > 0 and len(valid_chapters) > SKIP_COUNT:
        print(f"âœ‚ï¸ è·³éå‰ {SKIP_COUNT} å€‹é€£çµ (å¯èƒ½æ˜¯ç„¡æ•ˆç« ç¯€)...")
        valid_chapters = valid_chapters[SKIP_COUNT:]
    
    total = len(valid_chapters)
    print(f"ğŸ“– æº–å‚™ä¸‹è¼‰ {total} ç«  (æ…¢é€Ÿå®‰å…¨æ¨¡å¼)")

    filename = f"{NOVEL_NAME}.txt"
    with open(filename, "a", encoding="utf-8") as f:
        for index, (title, url) in enumerate(valid_chapters):
            print(f"[{index+1}/{total}] ä¸‹è¼‰: {title}")
            
            # å…§å±¤é‡è©¦æ©Ÿåˆ¶
            success = False
            for _ in range(3):
                try:
                    r = requests.get(url, headers=headers, timeout=15)
                    if r.status_code == 200:
                        r.encoding = 'utf-8'
                        page_soup = BeautifulSoup(r.text, 'html.parser')
                        # æŠ“å–å…§æ–‡
                        content = page_soup.select_one("#content") or \
                                  page_soup.select_one(".read-content") or \
                                  page_soup.select_one(".novelcontent")
                        
                        if content:
                            # æ¸…ç†
                            for tag in content(["script", "style", "div", "a", "iframe"]): 
                                tag.decompose()
                            text = content.get_text("\n\n", strip=True)
                            
                            f.write(f"\n\n{'='*20}\n{title}\n{'='*20}\n\n")
                            f.write(text)
                            f.flush()
                            success = True
                            break
                    time.sleep(2)
                except Exception:
                    time.sleep(2)
            
            if not success:
                print(f"   âš ï¸ ä¸‹è¼‰å¤±æ•—: {title}")
                f.write(f"\n\n[ç« ç¯€ {title} ä¸‹è¼‰å¤±æ•—]\n\n")

            # éš¨æ©Ÿå»¶é² 5~8 ç§’
            delay = random.uniform(3, 5)
            time.sleep(delay)

    print("\nâœ… å…¨éƒ¨å®Œæˆï¼")

if __name__ == "__main__":
    main()
